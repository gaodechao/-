**Accordion：**Adaptive Gradient Compression via Cirtiacal Learning Regime Identification

##### Abstract:

由于模型更新在计算节点上频繁传输，分布式模型训练存在通信瓶颈。

为了缓解这些瓶颈，从业者使用了梯度压缩技术，如稀疏化、量化、低秩更新等。这些技术通常需要选择一个静态压缩比，通常需要用户平衡模型精度和每次迭代加速之间的权衡

一个自适应压缩策略可以减少通信，同时保持最终的测试精度。

受最近对关键学习机制的发现，其中小的梯度误差可能对模型性能产生不可恢复的影响的启发，我们提出了一种简单而有效的自适应压缩算法 --ACCORDION。

虽然ACCORDION平均保持足够高的压缩率，但在关键学习体系中，它不过度压缩梯度，通过一个简单的基于梯度范数的标准检测来避免有害影响

##### Introducition

为了缓解梯度通信瓶颈，之前的工作提出了两种主要的方法：

1. 增加批量大小----这样每个worker在大批量上计算梯度，从而减少每epoch通信的频率
2. 通过有损耗梯度压缩----减少所通信的数据的大小。这两种方法都涉及到在性能和准确性之间进行权衡



- 使用大批量会导致最终精度下降是一种广泛观察到的现象:

  通过使用学习率预热，二阶信息，按分层的LR调谐

另一方面，当使用梯度压缩技术时，包括低精度的训练

只交换最大梯度坐标的TOPK方法或使用基于低秩的更新的方法

用户需要指定一个额外的超参数，以确定在训练开始之前的压缩或稀疏化程度。

